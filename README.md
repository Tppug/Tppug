#include #include #include #include #include using namespace std; #define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); } inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true) { if (code != cudaSuccess) { fprintf(stderr,"GPUassert: %s %s %d\n", cudaGetErrorString(code), file, line); if (abort) exit(code); } } template struct pitchedpointer { char *p; size_t pitch; __host__ __device__ pitchedpointer() {}; __host__ __device__ pitchedpointer(T0* _p, size_t _pitch) : p(reinterpret_cast(_p)), pitch(_pitch) {}; __device__ __host__ T& operator()(size_t i, size_t j) { T* v = reinterpret_cast(p + i*pitch); return v[j]; } __device__ __host__ const T& operator()(size_t i, size_t j) const { T* v = reinterpret_cast(p + i*pitch); return v[j]; } }; __global__ void ker(float * a, int m, int n, size_t pitch) { int row = threadIdx.x; pitchedpointer p(a, pitch); float4 f = p(row,1); printf("%d: %f %f %f %f\n", row, f.x, f.y, f.z, f.w); } int main() { int n=4,m=9; float *a=new float[n*m]; for(int i=0;i>>(dev_a, m, n, pitch); gpuErrchk( cudaPeekAtLastError() ); gpuErrchk( cudaDeviceSynchronize() ); cudaFree(dev_a); delete []a; cudaDeviceReset(); return 0; }st.shared.v4.f32 [writeS + 4*0*64], loadX0; st.shared.v4.f32 [writeS + 4*2*64], loadX2; st.shared.v4.f32 [writeS + 4*4*64], loadX4; st.shared.v4.f32 [writeS + 4*6*64], loadX6; // our loop needs one bar sync after share is loaded bar.sync 0; // Increment the track variables and swap shared buffers after the sync. // We know at this point that these registers are not tied up with any in flight memory op. track0 += ldx*8; track2 += ldx*8; track4 += ldx*8; track6 += ldx*8; writeS ^= 4*16*64; // Additional loop code omitted for clarity. 

} ``` Loading A and B by quad vector texture index:

Storing A and B into the shared address space with quad vectors:

tid = threadId.x; bx = blockId.x; by = blockId.y; blk = tid >= 32 ? by : bx; ldx = tid >= 32 ? ldb/4 : lda/4; tex = tid >= 32 ? texB : texA;

tid2 = (tid >> 4) & 1; tid15 = tid & 15; track0 = blk*64/4 + tid15 + (ldx * tid2); track2 = track0 + ldx*2; track4 = track0 + ldx*4; track6 = track0 + ldx*6; end = track0 + (k-8)*ldx; writeS = tid15*4*4 + tid2*64*4; writeS += tid >= 32 ? 2048 : 0; while (track0 < end) { tex.1d.v4.f32.s32 loadX0, [tex, track0]; tex.1d.v4.f32.s32 loadX2, [tex, track2]; tex.1d.v4.f32.s32 loadX4, [tex, track4]; tex.1d.v4.f32.s32 loadX6, [tex, track6]; st.shared.v4.f32 [writeS + 4*0*64], loadX0; st.shared.v4.f32 [writeS + 4*2*64], loadX2; st.shared.v4.f32 [writeS + 4*4*64], loadX4; st.shared.v4.f32 [writeS + 4*6*64], loadX6; // our loop needs one bar sync after share is loaded bar.sync 0; // Increment the track variables and swap shared buffers after the sync. // We know at this point that these registers are not tied up with any in flight memory op. track0 += ldx*8; track2 += ldx*8; track4 += ldx*8; track6 += ldx*8; writeS ^= 4*16*64;

__global__ void addKernelPTXv4(float4 *cc, const float4 *aa, const float4 *bb) { int i = threadIdx.x; cc[i].x = aa[i].x + bb[i].x; cc[i].y = aa[i].y + bb[i].y; cc[i].z = aa[i].z + bb[i].z; cc[i].w = aa[i].w + bb[i].w; } __global__ void addKernelPTXv4(float4 *cc, const float4 *aa, const float4 *bb) { asm( ".reg.f32 a1, a2, a3, a4;" ".reg.f32 b1, b2, b3, b4;" ".reg.f32 c1, c2, c3, c4;" ".reg.r32 r1;" ".reg.s64 rd1, rd2, rd3;" "mov.r32 r1, %tid.x;" "mul.s32 r1, r1, 16;" "add.s64 rd1, %0, r1;" float4 *vec4 = new float4[10]; float *vec = new float[4]; for(int i = 0; i < 10; i++) { for(int j = 0; j < 4; j++) vec[j] = j; vec4[i] = (float4){vec[0], vec[1], vec[2], vec[3]} }st.shared.v4.f32 [writeS + 4*0*64], loadX0; st.shared.v4.f32 [writeS + 4*2*64], loadX2; st.shared.v4.f32 [writeS + 4*4*64], loadX4; st.shared.v4.f32 [writeS + 4*6*64], loadX6; // our loop needs one bar sync after share is loaded bar.sync 0; // Increment the track variables and swap shared buffers after the sync. // We know at this point that these registers are not tied up with any in flight memory op. track0 += ldx*8; track2 += ldx*8; track4 += ldx*8; track6 += ldx*8; writeS ^= 4*16*64; // Additional loop code omitted for clarity. 

} ``` Loading A and B by quad vector texture index:

Storing A and B into the shared address space with quad vectors:


